import re
import json
import zipfile
from dataclasses import dataclass, asdict
from typing import List, Optional, Tuple, Dict
from io import BytesIO

import fitz  # PyMuPDF
import streamlit as st
import pandas as pd
from openpyxl import Workbook
from openpyxl.utils import get_column_letter


# =========================
# ç§‘ç›®ãƒ©ãƒ™ãƒ«ï¼ˆè¡¨ç¤ºã¯æ—¥æœ¬èªã§çµ±ä¸€ï¼‰
# =========================

SUBJECT_LABELS = {
    "zeimu": "ç§Ÿç¨",
    "zaimu": "è²¡å‹™",
    "kanri": "ç®¡ç†",
    "unknown": "ä¸æ˜",
}
LABEL_TO_CODE = {v: k for k, v in SUBJECT_LABELS.items()}

SUBJECT_CODES = ["zeimu", "zaimu", "kanri", "unknown"]
SUBJECT_LABEL_OPTIONS = [SUBJECT_LABELS[c] for c in SUBJECT_CODES]


# =========================
# æ­£è¦åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================

def normalize_dashes(s: str) -> str:
    return (
        s.replace("ï¼", "-")
         .replace("â€•", "-")
         .replace("âˆ’", "-")
    )

def normalize_rank(rank: Optional[str]) -> Optional[str]:
    if not rank:
        return None
    return rank.translate(str.maketrans({"ï¼¡": "A", "ï¼¢": "B", "ï¼£": "C"}))


# =========================
# æ­£è¦è¡¨ç¾ï¼ˆå…±é€šï¼šã¾ãšã¯ã“ã‚Œã§èµ°ã‚‰ã›ã‚‹ï¼‰
# =========================

EXAMPLE_RE = re.compile(
    r"ä¾‹é¡Œ\s*(?P<num>\d+)\s+(?P<title>.*?)(?:\s*ï¼ˆ(?P<rank>[A-Cï¼¡-ï¼£])ï¼‰)?\s*(?=\n|$)"
)

PAGE_REF_RE = re.compile(
    r"æ³•(?P<section>[^\s\-]+)-(?P<pageno>\d+)"
)


# =========================
# ç§‘ç›®åˆ¤å®šï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢æ–¹å¼ï¼ˆå …ã„ï¼‰
# =========================

SUBJECT_KEYWORDS: Dict[str, Dict[str, int]] = {
    "zeimu": {  # ç§Ÿç¨
        "æ³•äººç¨": 6, "æ³•äººç¨æ³•": 6, "ç§Ÿç¨": 5, "ç§Ÿç¨å…¬èª²": 6, "ç¨åŠ¹æœä¼šè¨ˆ": 4,
        "ç”³å‘Š": 3, "èª²ç¨æ‰€å¾—": 4, "åˆ¥è¡¨": 4, "å—å–é…å½“": 3, "å®Œå…¨æ”¯é…é–¢ä¿‚": 3,
        "å¯„é™„é‡‘": 3, "äº¤éš›è²»": 3, "æ¸›ä¾¡å„Ÿå´": 3,
    },
    "zaimu": {  # è²¡å‹™ä¼šè¨ˆ
        "è²¡å‹™ä¼šè¨ˆ": 6, "é€£çµ": 5, "ä¼æ¥­çµåˆ": 5, "é‡‘èå•†å“": 4, "é€€è·çµ¦ä»˜": 4,
        "åŒ…æ‹¬åˆ©ç›Š": 3, "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ãƒ•ãƒ­ãƒ¼": 3, "ä¼šè¨ˆæ–¹é‡": 3, "æ¸›æ": 3,
        "åç›Šèªè­˜": 3, "è³‡ç”£é™¤å»å‚µå‹™": 3, "ãƒªãƒ¼ã‚¹": 3,
    },
    "kanri": {  # ç®¡ç†ä¼šè¨ˆ
        "ç®¡ç†ä¼šè¨ˆ": 6, "CVP": 6, "æ¨™æº–åŸä¾¡": 5, "å·®ç•°åˆ†æ": 5, "ç›´æ¥åŸä¾¡": 5,
        "äºˆç®—": 4, "æ„æ€æ±ºå®š": 4, "è¨­å‚™æŠ•è³‡": 4, "åŸä¾¡è¨ˆç®—": 4, "åŸä¾¡ä¼ç”»": 4,
        "éƒ¨é–€åˆ¥": 3, "å†…éƒ¨æŒ¯æ›¿": 3,
    },
}

# ãƒ•ã‚¡ã‚¤ãƒ«åã¯å¼·ã„ãƒ’ãƒ³ãƒˆãªã®ã§ã€å°‘ã—å¼·ã‚ã«åŠ¹ã‹ã›ã‚‹
FILENAME_WEIGHT = 1.8

# ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚ã‚ŠãŒã¡ãªè¡¨è¨˜ã‚†ã‚Œã‚‚æ‹¾ã†ç”¨ï¼ˆåŠ ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
FILENAME_HINTS: Dict[str, Dict[str, int]] = {
    "zeimu": {
        "ç§Ÿç¨": 6, "ç¨æ³•": 6, "æ³•äººç¨": 6, "æ³•äººç¨æ³•": 6, "æ¶ˆè²»ç¨": 6, "æ‰€å¾—ç¨": 6,
    },
    "zaimu": {
        "è²¡å‹™": 6, "è²¡å‹™ä¼šè¨ˆ": 6, "ä¼šè¨ˆåŸºæº–": 4, "é€£çµ": 4, "ä¼æ¥­çµåˆ": 4,
    },
    "kanri": {
        "ç®¡ç†": 6, "ç®¡ç†ä¼šè¨ˆ": 6, "åŸä¾¡": 4, "CVP": 6, "å·®ç•°": 4, "äºˆç®—": 4,
    },
}


def _score_text(text: str, weights: Dict[str, Dict[str, int]], base_factor: float = 1.0) -> Dict[str, int]:
    scores = {k: 0 for k in ["zeimu", "zaimu", "kanri"]}
    t = text or ""
    for subj, kws in weights.items():
        s = 0
        for kw, w in kws.items():
            if kw and kw in t:
                s += w
        scores[subj] += int(round(s * base_factor))
    return scores


def detect_subject_from_doc(doc: fitz.Document, file_name: str) -> Tuple[str, Dict[str, int]]:
    # 1) TOC
    try:
        toc_text = " ".join([t for _, t, _ in doc.get_toc() if t]) or ""
    except Exception:
        toc_text = ""

    # 2) å…ˆé ­ãƒšãƒ¼ã‚¸æœ¬æ–‡ï¼ˆè»½ãï¼‰
    head_text = ""
    for i in range(min(8, len(doc))):
        try:
            head_text += (doc[i].get_text("text") or "") + "\n"
        except Exception:
            pass

    toc_text = normalize_dashes(toc_text).replace("\u00a0", " ")
    head_text = normalize_dashes(head_text).replace("\u00a0", " ")

    # 3) ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆå¼·ã„ãƒ’ãƒ³ãƒˆï¼‰
    fname = normalize_dashes(file_name or "").replace("\u00a0", " ")

    # ã‚¹ã‚³ã‚¢åˆç®—
    scores = {k: 0 for k in ["zeimu", "zaimu", "kanri"]}

    s1 = _score_text(toc_text, SUBJECT_KEYWORDS, base_factor=1.0)
    s2 = _score_text(head_text, SUBJECT_KEYWORDS, base_factor=1.0)
    s3 = _score_text(fname, FILENAME_HINTS, base_factor=FILENAME_WEIGHT)

    for k in scores:
        scores[k] = s1[k] + s2[k] + s3[k]

    # best
    best = max(scores, key=scores.get)
    ordered = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    top_score = ordered[0][1]
    second_score = ordered[1][1]

    # å®‰å…¨é‹è»¢ï¼šå¼±ã„/æ‹®æŠ—ãªã‚‰ä¸æ˜
    if top_score < 6 or (top_score - second_score) < 2:
        return "unknown", scores

    return best, scores


# =========================
# ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼šç« /ç¯€ï¼ˆã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ç”±æ¥ï¼‰
# =========================

@dataclass
class Chapter:
    no: int
    title: str
    start_page: int
    sections: List["Section"]

@dataclass
class Section:
    no: int
    title: str
    start_page: int

@dataclass
class ExampleItem:
    subject: str
    chapter_no: int
    chapter_title: str
    section_no: int
    section_title: str
    example_no: int
    title: str
    rank: Optional[str]
    page_ref: Optional[str]
    pdf_page: int
    source_pdf: str


def parse_outline(doc: fitz.Document) -> List[Chapter]:
    chapters: List[Chapter] = []
    current_chapter: Optional[Chapter] = None

    for level, title, page in doc.get_toc():
        title = normalize_dashes(title or "")

        if level == 1:
            m = re.search(r"ç¬¬(\d+)ç« \s*(.+)", title)
            if not m:
                continue
            current_chapter = Chapter(
                no=int(m.group(1)),
                title=m.group(2),
                start_page=page,
                sections=[]
            )
            chapters.append(current_chapter)

        elif level == 2 and current_chapter:
            m = re.search(r"ç¬¬(\d+)ç¯€\s*(.+)", title)
            if not m:
                continue
            current_chapter.sections.append(
                Section(
                    no=int(m.group(1)),
                    title=m.group(2),
                    start_page=page
                )
            )

    return chapters


def find_chapter_section(chapters: List[Chapter], pdf_page: int) -> Tuple[Optional[Chapter], Optional[Section]]:
    current_ch = None
    for ch in chapters:
        if pdf_page >= ch.start_page:
            current_ch = ch
    if not current_ch:
        return None, None

    current_sec = None
    for sec in current_ch.sections:
        if pdf_page >= sec.start_page:
            current_sec = sec

    return current_ch, current_sec


def extract_page_ref(text: str) -> Optional[str]:
    m = PAGE_REF_RE.search(text)
    if not m:
        return None
    return f"æ³•{m.group('section')}-{m.group('pageno')}"


# =========================
# æŠ½å‡ºï¼ˆå…±é€šï¼‰ï¼šç§‘ç›®ã”ã¨ã«å·®ã—æ›¿ãˆå¯èƒ½ãªè¨­è¨ˆ
# =========================

def extract_examples_common(pdf_bytes: bytes, subject_code: str, source_pdf: str) -> List[ExampleItem]:
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    chapters = parse_outline(doc)

    results: List[ExampleItem] = []
    last_page_ref: Optional[str] = None

    for i in range(len(doc)):
        page_no = i + 1
        page = doc[i]
        text = normalize_dashes(page.get_text("text") or "").replace("\u00a0", " ")

        found_ref = extract_page_ref(text)
        page_ref = found_ref or last_page_ref
        if found_ref:
            last_page_ref = found_ref

        chapter, section = find_chapter_section(chapters, page_no)
        if not chapter or not section:
            continue

        for m in EXAMPLE_RE.finditer(text):
            raw_title = re.sub(r"\s+", " ", (m.group("title") or "").strip())
            title = re.sub(r"\s*ï¼ˆ[A-Cï¼¡-ï¼£]ï¼‰\s*$", "", raw_title)

            results.append(
                ExampleItem(
                    subject=subject_code,
                    chapter_no=chapter.no,
                    chapter_title=chapter.title,
                    section_no=section.no,
                    section_title=section.title,
                    example_no=int(m.group("num")),
                    title=title,
                    rank=normalize_rank(m.group("rank")),
                    page_ref=page_ref,
                    pdf_page=page_no,
                    source_pdf=source_pdf,
                )
            )

    doc.close()
    return results


def extract_examples(pdf_bytes: bytes, subject_code: str, source_pdf: str) -> List[ExampleItem]:
    # ä»Šã¯å…±é€šæŠ½å‡ºå™¨ã€‚ç§‘ç›®åˆ¥ã«å¤‰ãˆãŸã„æ™‚ã¯ã“ã“ã§åˆ†å²ã™ã‚‹
    return extract_examples_common(pdf_bytes, subject_code, source_pdf)


# =========================
# ã‚½ãƒ¼ãƒˆï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é †ã«ä¾å­˜ã—ãªã„ï¼‰
# =========================

def sort_df(df: pd.DataFrame) -> pd.DataFrame:
    for col in ["chapter_no", "section_no", "example_no", "pdf_page"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    sort_cols = [c for c in ["subject", "chapter_no", "section_no", "pdf_page", "example_no"] if c in df.columns]
    return df.sort_values(by=sort_cols, kind="stable").reset_index(drop=True)


# =========================
# None é›†è¨ˆï¼ˆå…¨ã‚»ãƒ«ï¼‰
# =========================

def count_none_cells(df: pd.DataFrame) -> Tuple[int, Dict[str, int]]:
    na_mask = df.isna()
    none_str_mask = df.applymap(lambda x: isinstance(x, str) and x.strip().lower() == "none")
    mask = na_mask | none_str_mask
    total = int(mask.to_numpy().sum())
    per_col = mask.sum().astype(int).to_dict()
    return total, {k: int(v) for k, v in per_col.items()}


# =========================
# Excel å‡ºåŠ›ï¼ˆç§‘ç›®å˜ä½ï¼‰
#  - ç« ã”ã¨ã«ã‚·ãƒ¼ãƒˆ
#  - ã‚·ãƒ¼ãƒˆå†…ã§ç¯€åŒºåˆ‡ã‚Š
#  - ä¾‹é¡Œè¡Œï¼šA=é€£ç•ª, B=title, C=rank, D=page_refï¼ˆB/Cå…¥æ›¿ï¼‰
# =========================

def build_excel(df: pd.DataFrame, subject_code: str) -> BytesIO:
    wb = Workbook()
    wb.remove(wb.active)

    used_names = set()
    df_sorted = sort_df(df.copy())

    if "subject" in df_sorted.columns:
        df_sorted = df_sorted[df_sorted["subject"] == subject_code]

    if df_sorted.empty:
        buf = BytesIO()
        wb.save(buf)
        buf.seek(0)
        return buf

    for (chap_no, chap_title), chap_df in df_sorted.groupby(["chapter_no", "chapter_title"], sort=True):
        chap_no_int = int(chap_no) if pd.notna(chap_no) else 0
        base_name = f"{chap_no_int}ç«  {chap_title}"[:31]
        sheet_name = base_name

        idx = 2
        while sheet_name in used_names:
            suffix = f"_{idx}"
            sheet_name = (base_name[:31 - len(suffix)] + suffix)
            idx += 1

        used_names.add(sheet_name)
        ws = wb.create_sheet(title=sheet_name)

        row = 1
        for (sec_no, sec_title), sec_df in chap_df.groupby(["section_no", "section_title"], sort=True):
            sec_no_int = int(sec_no) if pd.notna(sec_no) else 0

            ws.cell(row=row, column=1, value=f"ç¬¬{sec_no_int}ç¯€")
            ws.cell(row=row, column=2, value=sec_title)
            row += 1

            counter = 1
            for _, r in sec_df.iterrows():
                ws.cell(row=row, column=1, value=counter)
                ws.cell(row=row, column=2, value=r.get("title"))     # B: title
                ws.cell(row=row, column=3, value=r.get("rank"))      # C: rank
                ws.cell(row=row, column=4, value=r.get("page_ref"))  # D: page_ref
                counter += 1
                row += 1

            row += 1

        widths = {1: 10, 2: 46, 3: 10, 4: 14}
        for col, w in widths.items():
            ws.column_dimensions[get_column_letter(col)].width = w

    buf = BytesIO()
    wb.save(buf)
    buf.seek(0)
    return buf


# =========================
# ZIP å‡ºåŠ›ï¼ˆç§‘ç›®åˆ¥xlsx + jsonï¼‰
# =========================

def build_zip(per_subject_dfs: Dict[str, pd.DataFrame]) -> BytesIO:
    zip_buf = BytesIO()
    with zipfile.ZipFile(zip_buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        for subj in ["zeimu", "zaimu", "kanri"]:
            df = per_subject_dfs.get(subj)
            if df is None or df.empty:
                continue

            label = SUBJECT_LABELS[subj]
            xlsx_buf = build_excel(df, subj)
            zf.writestr(f"{label}_examples.xlsx", xlsx_buf.getvalue())

            j = json.dumps(df.to_dict(orient="records"), ensure_ascii=False, indent=2)
            zf.writestr(f"{label}_examples.json", j)

        unk = per_subject_dfs.get("unknown")
        if unk is not None and not unk.empty:
            j = json.dumps(unk.to_dict(orient="records"), ensure_ascii=False, indent=2)
            zf.writestr("ä¸æ˜_examples.json", j)

        all_df = pd.concat([d for d in per_subject_dfs.values() if d is not None], ignore_index=True)
        all_df = sort_df(all_df)
        zf.writestr("ALL_examples.json", json.dumps(all_df.to_dict(orient="records"), ensure_ascii=False, indent=2))

    zip_buf.seek(0)
    return zip_buf


# =========================
# Streamlit UI
# =========================

st.set_page_config(page_title="CPA ä¾‹é¡ŒæŠ½å‡ºï¼ˆç§‘ç›®è‡ªå‹•åˆ¤å®šâ†’ç§‘ç›®åˆ¥å‡ºåŠ›ï¼‰", layout="wide")
st.title("CPAãƒ†ã‚­ã‚¹ãƒˆ ä¾‹é¡ŒæŠ½å‡ºãƒ„ãƒ¼ãƒ«ï¼ˆç§‘ç›®è‡ªå‹•åˆ¤å®š â†’ ä¸æ˜ã®ã¿æ‰‹ä¿®æ­£ â†’ ç§‘ç›®åˆ¥xlsxï¼‰")

uploaded_files = st.file_uploader(
    "PDFã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ï¼ˆè¤‡æ•°OKï¼‰",
    type=["pdf"],
    accept_multiple_files=True
)

if uploaded_files:
    file_rows = []
    file_bytes_map: Dict[str, bytes] = {}

    with st.spinner("ç§‘ç›®åˆ¤å®šä¸­â€¦ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‚‚åŠ ç‚¹ï¼‰"):
        for uf in uploaded_files:
            b = uf.read()
            file_bytes_map[uf.name] = b

            doc = fitz.open(stream=b, filetype="pdf")
            subj_code, scores = detect_subject_from_doc(doc, uf.name)
            doc.close()

            file_rows.append({
                "file_name": uf.name,
                "detected_subject": SUBJECT_LABELS[subj_code],
                "score_ç§Ÿç¨": scores.get("zeimu", 0),
                "score_è²¡å‹™": scores.get("zaimu", 0),
                "score_ç®¡ç†": scores.get("kanri", 0),
                "final_subject": SUBJECT_LABELS[subj_code],
            })

    st.success(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼š{len(uploaded_files)} ãƒ•ã‚¡ã‚¤ãƒ«")

    file_df = pd.DataFrame(file_rows)

    st.subheader("ç§‘ç›®åˆ¤å®šçµæœï¼ˆä¸æ˜ã ã‘ç›´ã›ã°OKï¼‰")

    with st.form("subject_form", clear_on_submit=False):
        edited_file_df = st.data_editor(
            file_df,
            use_container_width=True,
            num_rows="fixed",
            column_config={
                "final_subject": st.column_config.SelectboxColumn(
                    "final_subject",
                    options=SUBJECT_LABEL_OPTIONS
                ),
                "detected_subject": st.column_config.TextColumn("detected_subject", disabled=True),
                "file_name": st.column_config.TextColumn("file_name", disabled=True),
                "score_ç§Ÿç¨": st.column_config.NumberColumn("score_ç§Ÿç¨", disabled=True),
                "score_è²¡å‹™": st.column_config.NumberColumn("score_è²¡å‹™", disabled=True),
                "score_ç®¡ç†": st.column_config.NumberColumn("score_ç®¡ç†", disabled=True),
            },
            key="subject_editor"
        )
        run_extract = st.form_submit_button("è§£æå®Ÿè¡Œï¼ˆã“ã“ã§æŠ½å‡ºã‚¹ã‚¿ãƒ¼ãƒˆï¼‰")

    if run_extract:
        all_items = []
        with st.spinner("ä¾‹é¡ŒæŠ½å‡ºä¸­â€¦ï¼ˆå°‘ã—å¾…ã£ã¦ã­ï¼‰"):
            for _, r in edited_file_df.iterrows():
                fname = r["file_name"]
                subj_label = r["final_subject"]
                subj_code = LABEL_TO_CODE.get(subj_label, "unknown")

                b = file_bytes_map[fname]
                items = extract_examples(b, subj_code, fname)
                all_items.extend([asdict(x) for x in items])

        if not all_items:
            st.warning("ä¾‹é¡ŒãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä¾‹é¡Œã®è¡¨è¨˜ã‚†ã‚ŒãŒã‚ã‚‹ã‹ã‚‚ã€‚")
            st.stop()

        base_df = sort_df(pd.DataFrame(all_items))
        st.session_state["base_df"] = base_df
        st.session_state["edited_df"] = base_df.copy()

    if "base_df" in st.session_state:
        st.divider()
        st.subheader("ç·¨é›†ï¼ˆæœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼‰")

        with st.form("edit_form", clear_on_submit=False):
            edited_df = st.data_editor(
                st.session_state["edited_df"],
                use_container_width=True,
                num_rows="fixed",
                column_config={
                    "rank": st.column_config.SelectboxColumn("rank", options=[None, "A", "B", "C"]),
                    "title": st.column_config.TextColumn("title"),
                    "source_pdf": st.column_config.TextColumn("source_pdf", disabled=True),
                },
                key="main_editor"
            )
            update_check = st.form_submit_button("ãƒã‚§ãƒƒã‚¯æ›´æ–°ï¼ˆã“ã“ã§é›†è¨ˆï¼‰")

        if update_check:
            st.session_state["edited_df"] = sort_df(edited_df)

        current_df = st.session_state["edited_df"]

        st.subheader("ãƒã‚§ãƒƒã‚¯çµæœï¼ˆç¾åœ¨ï¼‰")
        total_none, per_col = count_none_cells(current_df)
        st.metric("å…¨ã‚»ãƒ«ã®æœªå…¥åŠ›ï¼ˆNoneï¼‰ä»¶æ•°", f"{total_none} å€‹")

        st.caption("åˆ—åˆ¥ã®æœªå…¥åŠ›ï¼ˆNoneï¼‰ä»¶æ•°")
        per_col_df = (
            pd.DataFrame([{"column": k, "none_count": int(v)} for k, v in per_col.items()])
            .sort_values("none_count", ascending=False)
            .reset_index(drop=True)
        )
        st.dataframe(per_col_df, use_container_width=True)

        if total_none == 0:
            st.success("ğŸ‰ æœªå…¥åŠ›ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‡ºåŠ›ã—ã¦OKã§ã™ã€‚")

        st.divider()
        st.subheader("æœ€çµ‚å‡ºåŠ›")

        per_subject = {}
        for s in SUBJECT_CODES:
            per_subject[s] = current_df[current_df["subject"] == s].copy() if "subject" in current_df.columns else pd.DataFrame()

        zip_buf = build_zip(per_subject)

        st.download_button(
            "ç§‘ç›®åˆ¥ZIPã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆxlsx + jsonï¼‰",
            data=zip_buf,
            file_name="CPA_examples_by_subject.zip",
            mime="application/zip"
        )

        st.download_button(
            "ALL_examples.jsonï¼ˆã¾ã¨ã‚ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=json.dumps(sort_df(current_df).to_dict(orient="records"), ensure_ascii=False, indent=2),
            file_name="ALL_examples.json",
            mime="application/json"
        )
